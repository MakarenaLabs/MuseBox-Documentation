{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MuseBox Server Documentation Welcome to MuseBox Server Documentation! MuseBox is a set of AI and Machine Learning models designed for extracting analytics out of a video or audio or audio/video source. MuseBox supports different workloads from edge AI inference to large datacenter AI inference tasks. MuseBox currently supports 5 different categories of AI tasks: Face Analysis for cutting edge analytics extraction of a single person People Analysis for crowds metadata extraction Audio Analysis for real-time segmentation and classification Object Analysis for extracting analytics regarding inanimate objects and text/draws Medical Analysis for US and MRI image analysis MuseBox runs on Zynq7000, Zynq Ultrascale+ MPSoC, ALVEO and VERSAL platforms. AMD Xilinx Solution Brief Join our community! References PYNQ : Python on Zynq is an open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips (APSoCs). Using the Python language and libraries, designers can exploit the benefits of programmable logic and microprocessors in Zynq to build more capable and exciting embedded systems. Licenses PYNQ License : BSD 3-Clause License MuseBox License : Commercial License","title":"Introduction"},{"location":"#musebox-server-documentation","text":"Welcome to MuseBox Server Documentation! MuseBox is a set of AI and Machine Learning models designed for extracting analytics out of a video or audio or audio/video source. MuseBox supports different workloads from edge AI inference to large datacenter AI inference tasks. MuseBox currently supports 5 different categories of AI tasks: Face Analysis for cutting edge analytics extraction of a single person People Analysis for crowds metadata extraction Audio Analysis for real-time segmentation and classification Object Analysis for extracting analytics regarding inanimate objects and text/draws Medical Analysis for US and MRI image analysis MuseBox runs on Zynq7000, Zynq Ultrascale+ MPSoC, ALVEO and VERSAL platforms. AMD Xilinx Solution Brief Join our community!","title":"MuseBox Server Documentation"},{"location":"#references","text":"PYNQ : Python on Zynq is an open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips (APSoCs). Using the Python language and libraries, designers can exploit the benefits of programmable logic and microprocessors in Zynq to build more capable and exciting embedded systems.","title":"References"},{"location":"#licenses","text":"PYNQ License : BSD 3-Clause License MuseBox License : Commercial License","title":"Licenses"},{"location":"setup/","text":"Setup MuseBox Server is provided as an executable file. There are 2 executable files: - MuseBoxVideo (for video tasks) - MuseBoxVideoWithIP (for video tasks both DPU and custom IPs) - MuseBoxAudio (for audio tasks) The executable file are in /usr/local/bin called MuseBoxVideo , MuseBoxVideoWithIP and MuseBoxAudio . For starting a server, simply run the preferred MuseBox command. For example, if you want to run MuseBox server for video task, run: # for websocket communication MuseBoxVideo websocket # for zmq communication MuseBoxVideo zmq Pre-built image You can download the pre-built image from here: DOWNLOAD NOW! You only need to flash the image on a SD card with common image burner. We suggest BalenaEtcher as SD image burner. License request In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, so you cannot use the license on a different node. For the license request, you need to create the license request file. To do that, simply run this command: license_request This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin Troubleshooting If you have this error when you run the MuseBox server or you request the license: Unknown device \"/dev/mmcblk0\": No such file or directory please launch this command: ln -s /dev/mmcblk1 /dev/mmcblk0 If the machine learning tasks seems to give wrong responses, please restart your board. If you need more help, contact us at: https://discord.gg/NpkTaJPAdp staff@makarenalabs.com","title":"Setup"},{"location":"setup/#setup","text":"MuseBox Server is provided as an executable file. There are 2 executable files: - MuseBoxVideo (for video tasks) - MuseBoxVideoWithIP (for video tasks both DPU and custom IPs) - MuseBoxAudio (for audio tasks) The executable file are in /usr/local/bin called MuseBoxVideo , MuseBoxVideoWithIP and MuseBoxAudio . For starting a server, simply run the preferred MuseBox command. For example, if you want to run MuseBox server for video task, run: # for websocket communication MuseBoxVideo websocket # for zmq communication MuseBoxVideo zmq","title":"Setup"},{"location":"setup/#pre-built-image","text":"You can download the pre-built image from here: DOWNLOAD NOW! You only need to flash the image on a SD card with common image burner. We suggest BalenaEtcher as SD image burner.","title":"Pre-built image"},{"location":"setup/#license-request","text":"In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, so you cannot use the license on a different node. For the license request, you need to create the license request file. To do that, simply run this command: license_request This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin","title":"License request"},{"location":"setup/#troubleshooting","text":"If you have this error when you run the MuseBox server or you request the license: Unknown device \"/dev/mmcblk0\": No such file or directory please launch this command: ln -s /dev/mmcblk1 /dev/mmcblk0 If the machine learning tasks seems to give wrong responses, please restart your board. If you need more help, contact us at: https://discord.gg/NpkTaJPAdp staff@makarenalabs.com","title":"Troubleshooting"},{"location":"api/cpp/","text":"C++ Example The C++ example is available on Github: https://github.com/MakarenaLabs/MuseBox-client-examples/tree/main/Cpp You can build the example directly on the board with this command: bash -x build_client.sh You can run the example on the board with this command: ./client <topic> <image | txt> [<image 2>] topic : a machine learning task of the following: FaceDetection\", \"FaceRecognition\", \"FaceLandmark\", \"AgeDetection\", \"GenderDetection\",\"GlassesDetection\",\"EmotionRecognition\", \"TextDetection\", \"LogoDetection\", \"LogoRecognition\", \"PeopleDetection\", \"ObjectDetection\", \"MonoDepth\",\"MonoDepth2\", \"MedicalDetection\", \"MedicalSegmentation\",\"HumanSegmentation\", \"ImagePortrait\", \"StochasticDifference\", \"ZeroCrossingRate\", \"SignalEnergy\", \"FFT\", \"STFT\", \"Monoaudio2Midi\" image : a local image path (like jpeg, png, jpg, etc image) txt : a txt file that contains values image 2 (optional) : a second local image path","title":"Example C++"},{"location":"api/cpp/#c-example","text":"The C++ example is available on Github: https://github.com/MakarenaLabs/MuseBox-client-examples/tree/main/Cpp You can build the example directly on the board with this command: bash -x build_client.sh You can run the example on the board with this command: ./client <topic> <image | txt> [<image 2>] topic : a machine learning task of the following: FaceDetection\", \"FaceRecognition\", \"FaceLandmark\", \"AgeDetection\", \"GenderDetection\",\"GlassesDetection\",\"EmotionRecognition\", \"TextDetection\", \"LogoDetection\", \"LogoRecognition\", \"PeopleDetection\", \"ObjectDetection\", \"MonoDepth\",\"MonoDepth2\", \"MedicalDetection\", \"MedicalSegmentation\",\"HumanSegmentation\", \"ImagePortrait\", \"StochasticDifference\", \"ZeroCrossingRate\", \"SignalEnergy\", \"FFT\", \"STFT\", \"Monoaudio2Midi\" image : a local image path (like jpeg, png, jpg, etc image) txt : a txt file that contains values image 2 (optional) : a second local image path","title":"C++ Example"},{"location":"api/general/","text":"API Overview The Theory MuseBox Server works via asynchronous messaging API based on different protocols. Every MuseBox Server implementation has an API system that supports 2 protocols: - ZeroMQ (aka ZMQ or \u00d8MQ or 0MQ), an embedded library for queue-based communication stack. - WebSocket , a standard two-way interactive communication protocol You can choose the communication protocol when you will start the MuseBox server, you simply pass to the second parameter of the executable the protocol ( zmq or websocket ). For example: ./MuseBoxAudio websocket If you need a point-to-point communication with 1 client or you have a web-based client, it is preferrable the WebSocket communication, otherwise is preferrable ZeroMQ. The message payload is the same in both protocols. The communication is based on the Publish - Subscribe classic pattern. The Senders of messages, called publishers (in our case, we can call it \"MuseBox Client\"), do not program the messages to be sent directly to specific receivers, called subscribers. Messages are published without the knowledge of what or if any subscriber of that knowledge exists. When MuseBox starts, it exposes its communication socket as a Subscriber. When it receives a message from a MuseBox Client, it responds to the Publisher at the socket that the MuseBox Client exposes. In order to do that, the MuseBox Client sends to the MuseBox Server in the message the address where it expects to receive the answer. The payload of the message is in JSON format. Let's see this example: A MuseBox Client sends this message to the MuseBox Server: { \"topic\": \"FaceDetection\", \"image\": [...], \"publisherQueue\": \"tcp://*:5000\" } the MuseBox Server receives the message. If the selected protocol is zeromq, according to the field \"publisherQueue\" will open a socket to the address tcp://*:5000 and will send the response there. the MuseBox Server sends the reponse to the MuseBox Client: { \"data\": [ { \"face_BB\": { \"height\":62, \"width\":51, \"x\":262, \"y\":97 } } ], \"publisherQueue\":\"tcp://*:5000\", \"status\":\"success\", \"topic\": \"FaceDetection\" } NB : in this documentation, the optional parameters will be written with this syntax: parameter?: <value> APIs The MuseBox Server listens in localhost (127.0.0.1) and in any IP (0.0.0.0) at port 9696. In WebSocket binding, the client simply must open a connection to 127.0.0.1:9696 , otherwise if the client is extern to the board, the connection string depends on the board's IP. For example, if the board has the IP 192.168.1.5 , the connection string is 192.168.1.5:9696 . In ZMQ socket binding, the Client will bind at this address for sending data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\"/\"input: <base64 image>/double array, \"publisherQueue\": <string> } Where: \"topic\" is the name of the Machine Learning task \"image\" is the OpenCV image in base64 format (string encoded in UTF8) \"publisherQueue\" is the address where the Client expects to receive the response \"only_face\" (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition. The MuseBox server response structure is the following: { \"data\": [ { \"<topic>_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], \"publisherQueue\": <string>, \"status\": <string>, \"topic\": <string> } Where: \"publisherQueue\" is the same string that the Client sent previously \"status\" is the status of the request (success / error) \"topic\" is the requested inference task \"data\" is the inference result in JSON Array, where: \"\\ _BB\" is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) NB: if the selected task has dependent tasks (e.g. Face Recognition requires Face Detection) and you have not setted only_face: true , the field \"data\" contains all the dependent task executed. For example, Face Recognition without only_face: true : { \"data\": { \"FaceDetection\": { \"data\": [ { \"face_BB\": { \"height\": 71, \"width\": 65, \"x\": 292, \"y\": 56 } } ], \"status\": \"success\", \"topic\": \"FaceDetection\" }, \"FaceRecognition\": [ { \"clientId\": \"1\", \"personFound\": \"unknown\", \"status\": \"success\", \"topic\": \"FaceRecognition\" } ] } } Face Analysis Face detection It detects the faces in a scene up to 32\u00d732 pixel. Request from Client: \"topic\": \"FaceDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"face_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } face_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Face landmarking Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"landmarks\": [196 int] } ] } landmarks corresponds to (x,y) couples for face landmark points Face recognition Given a face bounding box and a database of faces, it recognizes a face in a scene. You need to populate the directory /usr/local/bin/database/people with the photos of the desired people. Request from Client: \"topic\": \"FaceRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"personFound\": <string> } ] } personFound corresponds to the name of the person Glasses detection Given a face bounding box, it detects whether a person is wearing glasses or not. Request from Client: \"topic\": \"GlassesDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"glasses\": \"glasses\"/\"no glasses\" } ] } glasses is the result of inference (glasses / no glasses) Age Detection Given a face bounding box, it determines the age of a person. Request from Client: \"topic\": \"AgeDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"age\": <int> } ] } age corresponds to the age in an integer number between 0 - 100 Gender Detection Given a face bounding box, it determines if the person is female or male. Request from Client: \"topic\": \"GenderDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"gender\": \"man\"/\"woman\" } ] } gender corresponds to the gender of the person (man or woman) Emotion Recognition Given a face bounding box, it determines the facial expression/emotion of the person. Request from Client: \"topic\": \"EmotionRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"emotion\": \"angry\"/\"disgust\"/\"fear\"/\"happy\"/\"sad\"/\"surprise\"/\"neutral\" } ] } emotion corresponds to the facial emotion of the person People Analysis People detection It detects the people in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"PeopleDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"person_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } people_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Human Segmentation It segments the people present in a scene Request from Client: \"topic\": \"HumanSegmentation\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 320x320 double] } ], } image is an array of doubles that creates the mask to be applied on top of the original image Image Portrait It draws a black and white portrait of the people in the image. Request from Client: \"topic\": \"ImagePortrait\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 512x512 double] } ], } image is an array of doubles that creates a black and white portrait image Object Analysis Object detection It detects the objects in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"ObjectDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Logo detection It detects the objects in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"LogoDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"logo_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Logo Recognition Given a logo bounding box, it determines the brand name. You need to populate the directory /usr/local/bin/database/logos with the photos of the desired logos. Request from Client: \"topic\": \"LogoRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"logoFound\": <string> } ] } logoFound corresponds to the name of the logo Text Detection Given an image, it detects the texts inside of it. Request from Client: \"topic\": \"TextDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"text_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } ] } text_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Monocular Depth Given an image in input, it returns its depth estimation. Request from Client: \"topic\": \"MonoDepth\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 128x128 double] } ], } image is an array of doubles that represents the depth estimation of the input Monocular Depth #2 Given an image in input, it returns its depth estimation. Request from Client: \"topic\": \"MonoDepth2\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 192x640 double] } ], } image is an array of doubles that represents the depth estimation of the input Stochastic Difference Given two images in input, it returns how similiar those images are. Request from Client: \"topic\": \"StochasticDifference\" \"image\": \"base64 image\" \"image2\": \"base64 image\" Response from Server: { \"data\": [ { \"result\": double } ], } result is a double which value is a number from [0.0 - 1.0] describing how similiar the images in input are Medical Analysis Medical Detection Given an endoscopic image, it detects anything suspicious in it. Request from Client: \"topic\": \"MedicalDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"medical_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } ] } medical_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Medical Segmentation Given an endoscopic image, it segments anything suspicious in it. Request from Client: \"topic\": \"MedicalSegmentation\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"countour\": { \"Point\": { \"x\": <int>, \"y\": <int> } } ] } data contains all the points of the countours of the task, according to OpenCV format (Point: x, y) Audio Analysis Zero Crossing Rate Given a double array in input, it measures how many times the waveform crosses the zero axis. Request from Client: \"topic\": \"ZeroCrossingRate\" \"input\": [4410 double] Response from Server: { \"data\": [ \"result\": [double, double] ] } the first element of result is the number of times that the waveform crosses the zero axis, meanwhile the second one is the rate of the number of times that the waveform crosses the zero axis Signal Energy Given a double array in input, it returns the total magnitude of the signal. For audio signals, that roughly corresponds to how loud the signal is. Request from Client: \"topic\": \"SignalEnergy\" \"input\": [4410 double] Response from Server: { \"data\": [ \"result\": double ] } result is a double representing the total magnitude of the signal FFT Given a double array in input, it returns the FFT of the input. Request from Client: \"topic\": \"FFT\" \"input\": [2048 double] Response from Server: { \"data\": [ \"result\": [ 2048 double] ] } result is the number of the given FFT of the input STFT Given a double array in input, it returns the STFT of the input. Request from Client: \"topic\": \"STFT\" \"input\": [2048 double] Response from Server: { \"data\": [ \"result\": [ 2048 double] ] } result is the number of the given STFT of the input Mono Audio to Midi Given a double array in input, it returns the corresponding note in midi notation (from A0 to B0 of a monophonic audio, for example, if you pass to the function a sine wave of 440hz, the function returns A3). Request from Client: \"topic\": \"Monoaudio2Midi\" \"input\": [5512 double] Response from Server: { \"data\": [ \"result\": [string, string] ] } the result is the corresponding note in midi notation of the input","title":"API overview"},{"location":"api/general/#api-overview","text":"","title":"API Overview"},{"location":"api/general/#the-theory","text":"MuseBox Server works via asynchronous messaging API based on different protocols. Every MuseBox Server implementation has an API system that supports 2 protocols: - ZeroMQ (aka ZMQ or \u00d8MQ or 0MQ), an embedded library for queue-based communication stack. - WebSocket , a standard two-way interactive communication protocol You can choose the communication protocol when you will start the MuseBox server, you simply pass to the second parameter of the executable the protocol ( zmq or websocket ). For example: ./MuseBoxAudio websocket If you need a point-to-point communication with 1 client or you have a web-based client, it is preferrable the WebSocket communication, otherwise is preferrable ZeroMQ. The message payload is the same in both protocols. The communication is based on the Publish - Subscribe classic pattern. The Senders of messages, called publishers (in our case, we can call it \"MuseBox Client\"), do not program the messages to be sent directly to specific receivers, called subscribers. Messages are published without the knowledge of what or if any subscriber of that knowledge exists. When MuseBox starts, it exposes its communication socket as a Subscriber. When it receives a message from a MuseBox Client, it responds to the Publisher at the socket that the MuseBox Client exposes. In order to do that, the MuseBox Client sends to the MuseBox Server in the message the address where it expects to receive the answer. The payload of the message is in JSON format. Let's see this example: A MuseBox Client sends this message to the MuseBox Server: { \"topic\": \"FaceDetection\", \"image\": [...], \"publisherQueue\": \"tcp://*:5000\" } the MuseBox Server receives the message. If the selected protocol is zeromq, according to the field \"publisherQueue\" will open a socket to the address tcp://*:5000 and will send the response there. the MuseBox Server sends the reponse to the MuseBox Client: { \"data\": [ { \"face_BB\": { \"height\":62, \"width\":51, \"x\":262, \"y\":97 } } ], \"publisherQueue\":\"tcp://*:5000\", \"status\":\"success\", \"topic\": \"FaceDetection\" } NB : in this documentation, the optional parameters will be written with this syntax: parameter?: <value>","title":"The Theory"},{"location":"api/general/#apis","text":"The MuseBox Server listens in localhost (127.0.0.1) and in any IP (0.0.0.0) at port 9696. In WebSocket binding, the client simply must open a connection to 127.0.0.1:9696 , otherwise if the client is extern to the board, the connection string depends on the board's IP. For example, if the board has the IP 192.168.1.5 , the connection string is 192.168.1.5:9696 . In ZMQ socket binding, the Client will bind at this address for sending data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\"/\"input: <base64 image>/double array, \"publisherQueue\": <string> } Where: \"topic\" is the name of the Machine Learning task \"image\" is the OpenCV image in base64 format (string encoded in UTF8) \"publisherQueue\" is the address where the Client expects to receive the response \"only_face\" (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition. The MuseBox server response structure is the following: { \"data\": [ { \"<topic>_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], \"publisherQueue\": <string>, \"status\": <string>, \"topic\": <string> } Where: \"publisherQueue\" is the same string that the Client sent previously \"status\" is the status of the request (success / error) \"topic\" is the requested inference task \"data\" is the inference result in JSON Array, where: \"\\ _BB\" is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) NB: if the selected task has dependent tasks (e.g. Face Recognition requires Face Detection) and you have not setted only_face: true , the field \"data\" contains all the dependent task executed. For example, Face Recognition without only_face: true : { \"data\": { \"FaceDetection\": { \"data\": [ { \"face_BB\": { \"height\": 71, \"width\": 65, \"x\": 292, \"y\": 56 } } ], \"status\": \"success\", \"topic\": \"FaceDetection\" }, \"FaceRecognition\": [ { \"clientId\": \"1\", \"personFound\": \"unknown\", \"status\": \"success\", \"topic\": \"FaceRecognition\" } ] } }","title":"APIs"},{"location":"api/general/#face-analysis","text":"","title":"Face Analysis"},{"location":"api/general/#face-detection","text":"It detects the faces in a scene up to 32\u00d732 pixel. Request from Client: \"topic\": \"FaceDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"face_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } face_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Face detection"},{"location":"api/general/#face-landmarking","text":"Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"landmarks\": [196 int] } ] } landmarks corresponds to (x,y) couples for face landmark points","title":"Face landmarking"},{"location":"api/general/#face-recognition","text":"Given a face bounding box and a database of faces, it recognizes a face in a scene. You need to populate the directory /usr/local/bin/database/people with the photos of the desired people. Request from Client: \"topic\": \"FaceRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"personFound\": <string> } ] } personFound corresponds to the name of the person","title":"Face recognition"},{"location":"api/general/#glasses-detection","text":"Given a face bounding box, it detects whether a person is wearing glasses or not. Request from Client: \"topic\": \"GlassesDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"glasses\": \"glasses\"/\"no glasses\" } ] } glasses is the result of inference (glasses / no glasses)","title":"Glasses detection"},{"location":"api/general/#age-detection","text":"Given a face bounding box, it determines the age of a person. Request from Client: \"topic\": \"AgeDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"age\": <int> } ] } age corresponds to the age in an integer number between 0 - 100","title":"Age Detection"},{"location":"api/general/#gender-detection","text":"Given a face bounding box, it determines if the person is female or male. Request from Client: \"topic\": \"GenderDetection\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"gender\": \"man\"/\"woman\" } ] } gender corresponds to the gender of the person (man or woman)","title":"Gender Detection"},{"location":"api/general/#emotion-recognition","text":"Given a face bounding box, it determines the facial expression/emotion of the person. Request from Client: \"topic\": \"EmotionRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"emotion\": \"angry\"/\"disgust\"/\"fear\"/\"happy\"/\"sad\"/\"surprise\"/\"neutral\" } ] } emotion corresponds to the facial emotion of the person","title":"Emotion Recognition"},{"location":"api/general/#people-analysis","text":"","title":"People Analysis"},{"location":"api/general/#people-detection","text":"It detects the people in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"PeopleDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"person_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } people_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"People detection"},{"location":"api/general/#human-segmentation","text":"It segments the people present in a scene Request from Client: \"topic\": \"HumanSegmentation\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 320x320 double] } ], } image is an array of doubles that creates the mask to be applied on top of the original image","title":"Human Segmentation"},{"location":"api/general/#image-portrait","text":"It draws a black and white portrait of the people in the image. Request from Client: \"topic\": \"ImagePortrait\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 512x512 double] } ], } image is an array of doubles that creates a black and white portrait image","title":"Image Portrait"},{"location":"api/general/#object-analysis","text":"","title":"Object Analysis"},{"location":"api/general/#object-detection","text":"It detects the objects in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"ObjectDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Object detection"},{"location":"api/general/#logo-detection","text":"It detects the objects in a scene up to 64\u00d764 pixel. Request from Client: \"topic\": \"LogoDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"logo_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Logo detection"},{"location":"api/general/#logo-recognition","text":"Given a logo bounding box, it determines the brand name. You need to populate the directory /usr/local/bin/database/logos with the photos of the desired logos. Request from Client: \"topic\": \"LogoRecognition\" \"image\": \"base64 image\" \"only_face\"?: true Response from Server: { \"data\": [ { \"logoFound\": <string> } ] } logoFound corresponds to the name of the logo","title":"Logo Recognition"},{"location":"api/general/#text-detection","text":"Given an image, it detects the texts inside of it. Request from Client: \"topic\": \"TextDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"text_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } ] } text_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Text Detection"},{"location":"api/general/#monocular-depth","text":"Given an image in input, it returns its depth estimation. Request from Client: \"topic\": \"MonoDepth\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 128x128 double] } ], } image is an array of doubles that represents the depth estimation of the input","title":"Monocular Depth"},{"location":"api/general/#monocular-depth-2","text":"Given an image in input, it returns its depth estimation. Request from Client: \"topic\": \"MonoDepth2\" \"image\": \"base64 image\" Response from Server: { \"data\": [ { \"image\":[ 192x640 double] } ], } image is an array of doubles that represents the depth estimation of the input","title":"Monocular Depth #2"},{"location":"api/general/#stochastic-difference","text":"Given two images in input, it returns how similiar those images are. Request from Client: \"topic\": \"StochasticDifference\" \"image\": \"base64 image\" \"image2\": \"base64 image\" Response from Server: { \"data\": [ { \"result\": double } ], } result is a double which value is a number from [0.0 - 1.0] describing how similiar the images in input are","title":"Stochastic Difference"},{"location":"api/general/#medical-analysis","text":"","title":"Medical Analysis"},{"location":"api/general/#medical-detection","text":"Given an endoscopic image, it detects anything suspicious in it. Request from Client: \"topic\": \"MedicalDetection\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"medical_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } ] } medical_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Medical Detection"},{"location":"api/general/#medical-segmentation","text":"Given an endoscopic image, it segments anything suspicious in it. Request from Client: \"topic\": \"MedicalSegmentation\" \"image\": \"base64 image\" Response from Server: { \"data\": [ \"countour\": { \"Point\": { \"x\": <int>, \"y\": <int> } } ] } data contains all the points of the countours of the task, according to OpenCV format (Point: x, y)","title":"Medical Segmentation"},{"location":"api/general/#audio-analysis","text":"","title":"Audio Analysis"},{"location":"api/general/#zero-crossing-rate","text":"Given a double array in input, it measures how many times the waveform crosses the zero axis. Request from Client: \"topic\": \"ZeroCrossingRate\" \"input\": [4410 double] Response from Server: { \"data\": [ \"result\": [double, double] ] } the first element of result is the number of times that the waveform crosses the zero axis, meanwhile the second one is the rate of the number of times that the waveform crosses the zero axis","title":"Zero Crossing Rate"},{"location":"api/general/#signal-energy","text":"Given a double array in input, it returns the total magnitude of the signal. For audio signals, that roughly corresponds to how loud the signal is. Request from Client: \"topic\": \"SignalEnergy\" \"input\": [4410 double] Response from Server: { \"data\": [ \"result\": double ] } result is a double representing the total magnitude of the signal","title":"Signal Energy"},{"location":"api/general/#fft","text":"Given a double array in input, it returns the FFT of the input. Request from Client: \"topic\": \"FFT\" \"input\": [2048 double] Response from Server: { \"data\": [ \"result\": [ 2048 double] ] } result is the number of the given FFT of the input","title":"FFT"},{"location":"api/general/#stft","text":"Given a double array in input, it returns the STFT of the input. Request from Client: \"topic\": \"STFT\" \"input\": [2048 double] Response from Server: { \"data\": [ \"result\": [ 2048 double] ] } result is the number of the given STFT of the input","title":"STFT"},{"location":"api/general/#mono-audio-to-midi","text":"Given a double array in input, it returns the corresponding note in midi notation (from A0 to B0 of a monophonic audio, for example, if you pass to the function a sine wave of 440hz, the function returns A3). Request from Client: \"topic\": \"Monoaudio2Midi\" \"input\": [5512 double] Response from Server: { \"data\": [ \"result\": [string, string] ] } the result is the corresponding note in midi notation of the input","title":"Mono Audio to Midi"},{"location":"api/nocode/","text":"NoCode GUI The NoCode GUI allows you to execute in live the entire ML pipeline without any interaction with the backend software (in particular, you don't need to restart any process on the FPGA board, you just modify or compose your ML pipeline). You only need to start the MuseBox server with websocket protocol (depending on the task that you want to implement). A little demonstration: Getting started The software runs on a web browser on your PC (Chrome or Firefox). You need NodeJS on your PC (https://nodejs.org/en/) and python (https://www.python.org/downloads/). Just download the source code here: https://github.com/MakarenaLabs/MuseBox-NoCode-GUI and run these commands inside the root directory of the project: npm install python -m http.server 8000 The GUI will be available at address 127.0.0.1:8000 . Top menu On the top menu, there are 5 buttons: - save: save your current pipeline on the local storage of your browser - load: load your previous saved pipeline - download: download the pipeline as JSON file (if you want to use it on a custom client) - play/stop: start the pipeline or stop the pipeline - step: step to the next \"frame\" of the execution Nodes For adding a node on your current pipeline, click the right-click of the mouse. For customize a node, double-click on the node and a side menu will open. Every node has a number of input and output, identified graphically with a name (string) and a point. You can connect the output of a node to the input of another node simply dragging a line from one \"point\" to another with the left button mouse. For example, take a look at the \"face detection\" node: This node has 2 inputs (\"frame\" and \"person bounding box\") and 3 outputs (\"face bounding box\", \"post process\", \"logs\"). Those input are typed, so you cannot connect those inputs with outputs that have a different type (the GUI prevent you to connect the unelegible nodes). For example, a face detector takes in input an image and returns the bounding boxes. Those bounding boxes can be used by another task that can accept in input a bounding box (for example, the face landmarking). For instance, this is an elegible connection: You can notice that the input of face landmarking node is blinking and it is selectable, so when you will release the mouse the link will remain between the 2 nodes. Finally, this is a forbidden connection: You can notice that the input of the \"not\" node is not highlighted, because it takes in input a boolean value, not a bounding box. So, when you will release the mouse, the link will disappears. Startup At the startup, the \"configuration\" node will appear on the top-left of the grid. You can set the websocket url. At startup, the websocket url is setted as ws://127.0.0.1:8083 . If you are using the nocode GUI directly on the board, you can leave the websocket url as-is, meanwhile if you are using the GUI from your PC, simply replace the written IP with the board IP. Node menu When you right click on the main grid, a node menu will open. You can select different kind of nodes according to your purpose. The node types are the following: MuseBox Configuration: the configuration node MuseBox Input: the possible input of the system MuseBox Operators: operations useful for the pipeline (that runs on client CPU) MuseBox Ouptut: the possible output of the system MuseBox Tasks: machine learning task basic: basic operations and variable useful for the pipeline events: events generated during the pipeline logic: boolean operations math: mathematic operations (that runs on client CPU) math3d: mathematic 3D operations ( that runs on client CPU) string: string operations widget: graphical widget for the pipeline (buttons, gauges, knobs, etc) Node menu expansion MuseBox input MuseBox operations MuseBox output MuseBox Tasks: Face analysis tasks Object analysis tasks Basic nodes Events nodes Logic nodes Math nodes 3D Math nodes String nodes widget nodes","title":"NoCode GUI"},{"location":"api/nocode/#nocode-gui","text":"The NoCode GUI allows you to execute in live the entire ML pipeline without any interaction with the backend software (in particular, you don't need to restart any process on the FPGA board, you just modify or compose your ML pipeline). You only need to start the MuseBox server with websocket protocol (depending on the task that you want to implement). A little demonstration:","title":"NoCode GUI"},{"location":"api/nocode/#getting-started","text":"The software runs on a web browser on your PC (Chrome or Firefox). You need NodeJS on your PC (https://nodejs.org/en/) and python (https://www.python.org/downloads/). Just download the source code here: https://github.com/MakarenaLabs/MuseBox-NoCode-GUI and run these commands inside the root directory of the project: npm install python -m http.server 8000 The GUI will be available at address 127.0.0.1:8000 .","title":"Getting started"},{"location":"api/nocode/#top-menu","text":"On the top menu, there are 5 buttons: - save: save your current pipeline on the local storage of your browser - load: load your previous saved pipeline - download: download the pipeline as JSON file (if you want to use it on a custom client) - play/stop: start the pipeline or stop the pipeline - step: step to the next \"frame\" of the execution","title":"Top menu"},{"location":"api/nocode/#nodes","text":"For adding a node on your current pipeline, click the right-click of the mouse. For customize a node, double-click on the node and a side menu will open. Every node has a number of input and output, identified graphically with a name (string) and a point. You can connect the output of a node to the input of another node simply dragging a line from one \"point\" to another with the left button mouse. For example, take a look at the \"face detection\" node: This node has 2 inputs (\"frame\" and \"person bounding box\") and 3 outputs (\"face bounding box\", \"post process\", \"logs\"). Those input are typed, so you cannot connect those inputs with outputs that have a different type (the GUI prevent you to connect the unelegible nodes). For example, a face detector takes in input an image and returns the bounding boxes. Those bounding boxes can be used by another task that can accept in input a bounding box (for example, the face landmarking). For instance, this is an elegible connection: You can notice that the input of face landmarking node is blinking and it is selectable, so when you will release the mouse the link will remain between the 2 nodes. Finally, this is a forbidden connection: You can notice that the input of the \"not\" node is not highlighted, because it takes in input a boolean value, not a bounding box. So, when you will release the mouse, the link will disappears.","title":"Nodes"},{"location":"api/nocode/#startup","text":"At the startup, the \"configuration\" node will appear on the top-left of the grid. You can set the websocket url. At startup, the websocket url is setted as ws://127.0.0.1:8083 . If you are using the nocode GUI directly on the board, you can leave the websocket url as-is, meanwhile if you are using the GUI from your PC, simply replace the written IP with the board IP.","title":"Startup"},{"location":"api/nocode/#node-menu","text":"When you right click on the main grid, a node menu will open. You can select different kind of nodes according to your purpose. The node types are the following: MuseBox Configuration: the configuration node MuseBox Input: the possible input of the system MuseBox Operators: operations useful for the pipeline (that runs on client CPU) MuseBox Ouptut: the possible output of the system MuseBox Tasks: machine learning task basic: basic operations and variable useful for the pipeline events: events generated during the pipeline logic: boolean operations math: mathematic operations (that runs on client CPU) math3d: mathematic 3D operations ( that runs on client CPU) string: string operations widget: graphical widget for the pipeline (buttons, gauges, knobs, etc)","title":"Node menu"},{"location":"api/nocode/#node-menu-expansion","text":"MuseBox input MuseBox operations MuseBox output MuseBox Tasks: Face analysis tasks Object analysis tasks Basic nodes Events nodes Logic nodes Math nodes 3D Math nodes String nodes widget nodes","title":"Node menu expansion"},{"location":"api/python/","text":"Python Example The Python example is available on Github: https://github.com/MakarenaLabs/MuseBox-client-examples/tree/main/Python There are 3 example: a python script ( musebox_client.py ) that implements a MuseBox client based on zmq communication (like the C++ example) a python script ( musebox_client.py ) that implements a MuseBox client based on websocket communication (like the C++ example) a jupyter notebook script ( client.ipynb ) that implements a MuseBox client based on zmq communication","title":"Example Python"},{"location":"api/python/#python-example","text":"The Python example is available on Github: https://github.com/MakarenaLabs/MuseBox-client-examples/tree/main/Python There are 3 example: a python script ( musebox_client.py ) that implements a MuseBox client based on zmq communication (like the C++ example) a python script ( musebox_client.py ) that implements a MuseBox client based on websocket communication (like the C++ example) a jupyter notebook script ( client.ipynb ) that implements a MuseBox client based on zmq communication","title":"Python Example"}]}