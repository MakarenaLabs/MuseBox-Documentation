{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MuseBox Server Documentation Welcome to MuseBox Server Documentation! MuseBox is a set of AI and Machine Learning models designed for extracting analytics out of a video or audio or audio/video source. MuseBox supports different workloads from edge AI inference to large datacenter AI inference tasks. MuseBox currently supports 5 different categories of AI tasks: Face Analysis for cutting edge analytics extraction of a single person People Analysis for crowds metadata extraction Audio Analysis for real-time segmentation and classification Object Analysis for extracting analytics regarding inanimate objects and text/draws Medical Analysis for US and MRI image analysis MuseBox runs on Zynq7000, Zynq Ultrascale+ MPSoC, ALVEO and VERSAL platforms. AMD Xilinx Solution Brief References PYNQ : Python on Zynq is an open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips (APSoCs). Using the Python language and libraries, designers can exploit the benefits of programmable logic and microprocessors in Zynq to build more capable and exciting embedded systems. Licenses PYNQ License : BSD 3-Clause License MuseBox License : Commercial License","title":"Introduction"},{"location":"#musebox-server-documentation","text":"Welcome to MuseBox Server Documentation! MuseBox is a set of AI and Machine Learning models designed for extracting analytics out of a video or audio or audio/video source. MuseBox supports different workloads from edge AI inference to large datacenter AI inference tasks. MuseBox currently supports 5 different categories of AI tasks: Face Analysis for cutting edge analytics extraction of a single person People Analysis for crowds metadata extraction Audio Analysis for real-time segmentation and classification Object Analysis for extracting analytics regarding inanimate objects and text/draws Medical Analysis for US and MRI image analysis MuseBox runs on Zynq7000, Zynq Ultrascale+ MPSoC, ALVEO and VERSAL platforms. AMD Xilinx Solution Brief","title":"MuseBox Server Documentation"},{"location":"#references","text":"PYNQ : Python on Zynq is an open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips (APSoCs). Using the Python language and libraries, designers can exploit the benefits of programmable logic and microprocessors in Zynq to build more capable and exciting embedded systems.","title":"References"},{"location":"#licenses","text":"PYNQ License : BSD 3-Clause License MuseBox License : Commercial License","title":"Licenses"},{"location":"setup/","text":"Setup MuseBox Server is provided as an executable file. If you will use the pre-built image, the executable file is in ~/makarenalabs/musebox-demo called MuseBox . If you will install MuseBox on your petalinux image, the executable file is in /usr/local/bin , called MuseBoxServer . Pre-built image If you want to use the pre-built image, you only need to flash the image on a SD card with common image burner. We suggest BalenaEtcher as SD image burner. Installation on your Petalinux image If you want to install MuseBox on top of your current petalinux image, follow those steps Prerequisites Petalinux 2021.1 image PYNQ v2.7 (following steps) Valid license provided by MakarenaLabs SRL Step 1: PYNQ installation To install PYNQ you can simply use the provided PRM package with the follwing command: dnf install pynq-2.7-1.fc30.noarch.rpm After that, to apply all changes you have to reboot the board and wait a couple of seconds before start using PYNQ on Kria board. Jupyter notebooks are available on 9090 port, so you can reach them using the URL http://board_ip:9090 . Step 2: MuseBox server installation To start using MuseBox Server you have to install it by an RPM package with the following command (it takes about 20 mins): dnf install musebox_server-2021.1_1.4-1.fc30.noarch.rpm Configuration First of all, you need to check if the file dpu.xclbin is visible to the system, so check if the file is in the path /media/sd-mmcblk0p1/ . If it is not true, you have to link the DPU file ( dpu.xclbin ) into a specific directory inside /media folder. You can do this with the following: mkdir -p /media/sd-mmcblk0p1/ ln -s /run/media/mmcblk1p1/dpu.xclbin /media/sd-mmcblk0p1/ At your discretion, you can also copy the file in the directory, instead of link it (you have to change the ln -s command with the cp one). Once the DPU file is located in the right directory, you can copy your license.lic file inside the MuseBox Server installation directory ( /usr/local/bin ). License request In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, composed by a specific internal ID, MAC address and SD Card/MMC ID, so you cannot use the license on a different node. For the license request, you need to create the license request file. For doing that, simply run this command: /usr/local/bin/request_license This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin","title":"Setup"},{"location":"setup/#setup","text":"MuseBox Server is provided as an executable file. If you will use the pre-built image, the executable file is in ~/makarenalabs/musebox-demo called MuseBox . If you will install MuseBox on your petalinux image, the executable file is in /usr/local/bin , called MuseBoxServer .","title":"Setup"},{"location":"setup/#pre-built-image","text":"If you want to use the pre-built image, you only need to flash the image on a SD card with common image burner. We suggest BalenaEtcher as SD image burner.","title":"Pre-built image"},{"location":"setup/#installation-on-your-petalinux-image","text":"If you want to install MuseBox on top of your current petalinux image, follow those steps","title":"Installation on your Petalinux image"},{"location":"setup/#prerequisites","text":"Petalinux 2021.1 image PYNQ v2.7 (following steps) Valid license provided by MakarenaLabs SRL","title":"Prerequisites"},{"location":"setup/#step-1-pynq-installation","text":"To install PYNQ you can simply use the provided PRM package with the follwing command: dnf install pynq-2.7-1.fc30.noarch.rpm After that, to apply all changes you have to reboot the board and wait a couple of seconds before start using PYNQ on Kria board. Jupyter notebooks are available on 9090 port, so you can reach them using the URL http://board_ip:9090 .","title":"Step 1: PYNQ installation"},{"location":"setup/#step-2-musebox-server-installation","text":"To start using MuseBox Server you have to install it by an RPM package with the following command (it takes about 20 mins): dnf install musebox_server-2021.1_1.4-1.fc30.noarch.rpm","title":"Step 2: MuseBox server installation"},{"location":"setup/#configuration","text":"First of all, you need to check if the file dpu.xclbin is visible to the system, so check if the file is in the path /media/sd-mmcblk0p1/ . If it is not true, you have to link the DPU file ( dpu.xclbin ) into a specific directory inside /media folder. You can do this with the following: mkdir -p /media/sd-mmcblk0p1/ ln -s /run/media/mmcblk1p1/dpu.xclbin /media/sd-mmcblk0p1/ At your discretion, you can also copy the file in the directory, instead of link it (you have to change the ln -s command with the cp one). Once the DPU file is located in the right directory, you can copy your license.lic file inside the MuseBox Server installation directory ( /usr/local/bin ).","title":"Configuration"},{"location":"setup/#license-request","text":"In order to obtain the license, you need to request the node locker license. The license is valid for a specific board, composed by a specific internal ID, MAC address and SD Card/MMC ID, so you cannot use the license on a different node. For the license request, you need to create the license request file. For doing that, simply run this command: /usr/local/bin/request_license This command generates the file license_request.req in the path /usr/local . You need to send to us the file via email at staff@makarenalabs.com with the subject MuseBox License request . Our internal system will check if the email is associated to a valid customer, then, according to your signed contract, the system will respond to you with the license file, called license.lic . When you receive the license file, you need to place the file in this path: /usr/local/bin","title":"License request"},{"location":"api/cpp/","text":"C++ Example This example is available on the following link: https://s3.eu-central-1.wasabisys.com/musebox/musebox-client-examples-1.0.0.zip #include <zmq.h> #include <string> #include <vector> #include <opencv2/opencv.hpp> #include <nlohmann/json.hpp> typedef unsigned char BYTE; static const std::string base64_chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"abcdefghijklmnopqrstuvwxyz\" \"0123456789+/\"; static inline bool is_base64(unsigned char c) { return (isalnum(c) || (c == '+') || (c == '/')); } std::string base64_encode(BYTE const *buf, unsigned int bufLen) { std::string ret; int i = 0; int j = 0; BYTE char_array_3[3]; BYTE char_array_4[4]; while (bufLen--) { char_array_3[i++] = *(buf++); if (i == 3) { char_array_4[0] = (char_array_3[0] & 0xfc) >> 2; char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4); char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6); char_array_4[3] = char_array_3[2] & 0x3f; for (i = 0; (i < 4); i++) ret += base64_chars[char_array_4[i]]; i = 0; } } if (i) { for (j = i; j < 3; j++) char_array_3[j] = '\\0'; char_array_4[0] = (char_array_3[0] & 0xfc) >> 2; char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4); char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6); char_array_4[3] = char_array_3[2] & 0x3f; for (j = 0; (j < i + 1); j++) ret += base64_chars[char_array_4[j]]; while ((i++ < 3)) ret += '='; } return ret; } int main(int argc, char *argv[]) { // Check args if (argc != 3) { std::cout << \"Usage of client demo: ./client topic image\" << std::endl; return -1; } // Check topic std::vector<std::string> valid_topics { \"FaceDetection\", \"FaceRecognition\", \"FaceLandmark\", \"EyeBlink\", \"AgeDetection\", \"GenderDetection\", \"GlassesDetection\", \"EmotionRecognition\", \"TextDetection\", \"LogoDetection\" }; // For face recognition the database is located on /usr/local/bin/database/people if(!(std::find(std::begin(valid_topics), std::end(valid_topics), argv[1]) != valid_topics.end())){ std::cout << \"The topic is not valid.\" << std::endl; return -1; } // set the ZMQ context void *context = zmq_ctx_new(); void *publisher = zmq_socket(context, ZMQ_PUB); assert (publisher); // bind to the queue as publisher int bind = zmq_bind(publisher, \"tcp://127.0.0.1:9696\"); if(bind < 0){ unsigned int error_code = zmq_errno(); printf(\"zmq_bind server ctx error: %u, %s\\n\", error_code, zmq_strerror(error_code)); assert (bind == 0); } // create a cv::Mat that represents the image cv::Mat img = cv::imread(argv[2]); cv::resize(img, img, cv::Size(640, 360)); // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".png\", img, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); nlohmann::json messageToSend = { {\"clientId\", \"1\"}, // client ID {\"topic\", argv[1]}, // Machine Learning task {\"publisherQueue\", \"tcp://*:5556\"}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; // send the message int output = zmq_send(publisher, messageToSend.dump().c_str(), messageToSend.dump().size(), 0); if(output < 0){ unsigned int error_code = zmq_errno(); printf(\"server ctx error: %u, %s\\n\", error_code, zmq_strerror(error_code)); assert (output == 0); } std::cout << \"sent \" << std::endl; // open a socket for receive the response void *socket = zmq_socket(context, ZMQ_SUB); int connect = zmq_connect(socket, \"tcp://127.0.0.1:5556\"); int rc = zmq_setsockopt(socket, ZMQ_SUBSCRIBE, \"\", 0); long length = 2400000; char *buf = new char[length]; std::fill_n(buf, length, 0); std::cout << \"######################## Waiting for server ################################\" << std::endl; // blocking receive for ZMQ int nbytes = zmq_recv(socket, buf, length, 0); std::string str_msg = buf; // parse the json response (from string to object) auto message = nlohmann::json::parse(str_msg); std::cout << message << std::endl; return 0; }","title":"Example C++"},{"location":"api/cpp/#c-example","text":"This example is available on the following link: https://s3.eu-central-1.wasabisys.com/musebox/musebox-client-examples-1.0.0.zip #include <zmq.h> #include <string> #include <vector> #include <opencv2/opencv.hpp> #include <nlohmann/json.hpp> typedef unsigned char BYTE; static const std::string base64_chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"abcdefghijklmnopqrstuvwxyz\" \"0123456789+/\"; static inline bool is_base64(unsigned char c) { return (isalnum(c) || (c == '+') || (c == '/')); } std::string base64_encode(BYTE const *buf, unsigned int bufLen) { std::string ret; int i = 0; int j = 0; BYTE char_array_3[3]; BYTE char_array_4[4]; while (bufLen--) { char_array_3[i++] = *(buf++); if (i == 3) { char_array_4[0] = (char_array_3[0] & 0xfc) >> 2; char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4); char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6); char_array_4[3] = char_array_3[2] & 0x3f; for (i = 0; (i < 4); i++) ret += base64_chars[char_array_4[i]]; i = 0; } } if (i) { for (j = i; j < 3; j++) char_array_3[j] = '\\0'; char_array_4[0] = (char_array_3[0] & 0xfc) >> 2; char_array_4[1] = ((char_array_3[0] & 0x03) << 4) + ((char_array_3[1] & 0xf0) >> 4); char_array_4[2] = ((char_array_3[1] & 0x0f) << 2) + ((char_array_3[2] & 0xc0) >> 6); char_array_4[3] = char_array_3[2] & 0x3f; for (j = 0; (j < i + 1); j++) ret += base64_chars[char_array_4[j]]; while ((i++ < 3)) ret += '='; } return ret; } int main(int argc, char *argv[]) { // Check args if (argc != 3) { std::cout << \"Usage of client demo: ./client topic image\" << std::endl; return -1; } // Check topic std::vector<std::string> valid_topics { \"FaceDetection\", \"FaceRecognition\", \"FaceLandmark\", \"EyeBlink\", \"AgeDetection\", \"GenderDetection\", \"GlassesDetection\", \"EmotionRecognition\", \"TextDetection\", \"LogoDetection\" }; // For face recognition the database is located on /usr/local/bin/database/people if(!(std::find(std::begin(valid_topics), std::end(valid_topics), argv[1]) != valid_topics.end())){ std::cout << \"The topic is not valid.\" << std::endl; return -1; } // set the ZMQ context void *context = zmq_ctx_new(); void *publisher = zmq_socket(context, ZMQ_PUB); assert (publisher); // bind to the queue as publisher int bind = zmq_bind(publisher, \"tcp://127.0.0.1:9696\"); if(bind < 0){ unsigned int error_code = zmq_errno(); printf(\"zmq_bind server ctx error: %u, %s\\n\", error_code, zmq_strerror(error_code)); assert (bind == 0); } // create a cv::Mat that represents the image cv::Mat img = cv::imread(argv[2]); cv::resize(img, img, cv::Size(640, 360)); // create the vector that \"flats\" the cv::Mat in a single dimension std::vector<uchar> img_buf; cv::imencode(\".png\", img, img_buf); auto *enc_msg = reinterpret_cast<unsigned char *>(img_buf.data()); std::string encoded = base64_encode(enc_msg, img_buf.size()); nlohmann::json messageToSend = { {\"clientId\", \"1\"}, // client ID {\"topic\", argv[1]}, // Machine Learning task {\"publisherQueue\", \"tcp://*:5556\"}, // where the MuseBox subscriber will respond {\"image\", encoded}, // image to infer }; // send the message int output = zmq_send(publisher, messageToSend.dump().c_str(), messageToSend.dump().size(), 0); if(output < 0){ unsigned int error_code = zmq_errno(); printf(\"server ctx error: %u, %s\\n\", error_code, zmq_strerror(error_code)); assert (output == 0); } std::cout << \"sent \" << std::endl; // open a socket for receive the response void *socket = zmq_socket(context, ZMQ_SUB); int connect = zmq_connect(socket, \"tcp://127.0.0.1:5556\"); int rc = zmq_setsockopt(socket, ZMQ_SUBSCRIBE, \"\", 0); long length = 2400000; char *buf = new char[length]; std::fill_n(buf, length, 0); std::cout << \"######################## Waiting for server ################################\" << std::endl; // blocking receive for ZMQ int nbytes = zmq_recv(socket, buf, length, 0); std::string str_msg = buf; // parse the json response (from string to object) auto message = nlohmann::json::parse(str_msg); std::cout << message << std::endl; return 0; }","title":"C++ Example"},{"location":"api/general/","text":"API Overview The Theory MuseBox Server works via asynchronous messaging API based on different protocols. Every MuseBox Server implementation has a common API based on ZeroMQ (aka ZMQ or \u00d8MQ or 0MQ), an embedded library for queue-based communication stack. The communication is based on the Publish - Subscribe classic pattern. The Senders of messages, called publishers (in our case, we can call it \"MuseBox Client\"), do not program the messages to be sent directly to specific receivers, called subscribers. Messages are published without the knowledge of what or if any subscriber of that knowledge exists. When MuseBox starts, exposes its communication socket as a Subscriber. When it receives a message from a MuseBox Client, it responds to the Publisher to the socket that the MuseBox Client exposes. In order to to that, the MuseBox Client send to the MuseBox Server message the address where it expects to receive the answer. The payload of the message is in JSON format. Let's see this example: A MuseBox Client send this message to the MuseBox Server: { \"topic\": \"FaceDetection\", \"image\": [...], \"imageHeight\": 250, \"imageWidth\": 250, \"publisherQueue\": \"tcp://*:5000\" } the MuseBox Server receives the message, and according to the field \"publisherQueue\", will open a socket to the address tcp://*:5000 and will send the response there. the MuseBox Server sends the reponse to the MuseBox Client: { \"data\": [ { \"face_BB\": { \"height\":62, \"width\":51, \"x\":262, \"y\":97 } } ], \"execution_time\":4.957312, \"publisherQueue\":\"tcp://*:5000\", \"status\":\"success\", \"topic\": \"FaceDetection\" } NB : in this documentation, the optional parameters will be written with this syntax: parameter?: <value> APIs The MuseBox Server listens in localhost (127.0.0.1) at port 9696 in TCP protocol. According to that, in ZMQ socket binding, the Client will bind at this address for sending data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\": <base64 image>, \"imageHeight\": <int>, \"imageWidth\": <int>, \"publisherQueue\": <string> } Where: \"topic\" is the name of the Machine Learning task \"image\" is the OpenCV image in base64 format (string encoded in UTF8) \"imageHeight\" is the height of the current image \"imageWidth\" is the width of the current image \"publisherQueue\" is the address where the Client expects to receive the response \"only_face\" (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition. The MuseBox server response structure is the following: { \"data\": [ { \"<topic>_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], \"execution_time\": <double>, \"publisherQueue\": <string>, \"status\": <string>, \"topic\": <string> } Where: \"execution_time\" is the inference time on FPGA \"publisherQueue\" is the same string that the Client sent previously \"status\" is the status of the request (success / error) \"topic\" is the requested inference task \"data\" is the inference result in JSON Array, where: \"\\ _BB\" is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) NB: if the selected task has dependent tasks (e.g. Face Recognition requires Face Detection) and you have not setted only_face: true , the field \"data\" contains all the dependent task executed. For example, Face Recognition without only_face: true : { \"data\": { \"FaceDetection\": { \"data\": [ { \"face_BB\": { \"height\": 71, \"width\": 65, \"x\": 292, \"y\": 56 } } ], \"status\": \"success\", \"topic\": \"FaceDetection\" }, \"FaceRecognition\": [ { \"clientId\": \"1\", \"personFound\": \"unknown\", \"status\": \"success\", \"topic\": \"FaceRecognition\" } ] } } Face Analysis Face detection It detects the faces in a scene up to 32\u00d732 pixel Request from Client: \"topic\": \"FaceDetection\" Response from Server: { \"data\": [ { \"face_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } face_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Face landmarking Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"only_face\"?: true Response from Server: { \"data\": [ { \"landmarks\": [196 int] } ] } landmarks corresponds to (x,y) couples for face landmark points Face recognition Given a face bounding box and a database of faces, it recognizes a face in a scene You need to populate the directory /usr/local/bin/database/people with the photos of the desired people. Request from Client: \"topic\": \"FaceRecognition\" \"only_face\"?: true Response from Server: { \"data\": [ { \"personFound\": <string> } ] } personFound corresponds to the name of the person Glasses detection Given a face bounding box, it detects whether a person wear glasses or not Request from Client: \"topic\": \"GlassesDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"glasses\": \"glasses\"/\"no glasses\" } ] } glasses is the result of inference (glasses / no glasses) Eye Blink Detection Given a face bounding box, it detects where are the eyes and it determines if they are blinking or not Request from Client: \"topic\": \"EyeBlink\" \"only_face\"?: true Response from Server: { \"data\": [ { \"eyeState\": \"opened\"/\"closed\" } ] } eyeState is the result of inference (opened eyes / closed eyes) Age Detection Given a face bounding box, it determines the age of a person Request from Client: \"topic\": \"AgeDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"age\": <int> } ] } age corresponds to the age in an integer number between 0 - 100 Gender Detection Given a face bounding box, it determines if the person is female or male Request from Client: \"topic\": \"GenderDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"gender\": \"man\"/\"woman\" } ] } gender corresponds to the gender of the person (man or woman) People Analysis People detection It detects the people in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"PeopleDetection\" Response from Server: { \"data\": [ { \"person_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } people_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Object Analysis Object detection It detects the objects in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"ObjectDetection\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Logo detection It detects the objects in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"LogoDetection\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) Logo Recognition Given a logo bounding box, it determines the brand name You need to populate the directory /usr/local/bin/database/logo with the photos of the desired logos. Request from Client: \"topic\": \"LogoRecognition\" \"only_face\"?: true Response from Server: { \"data\": [ { \"logoFound\": <string> } ] } logoFound corresponds to the name of the logo","title":"API overview"},{"location":"api/general/#api-overview","text":"","title":"API Overview"},{"location":"api/general/#the-theory","text":"MuseBox Server works via asynchronous messaging API based on different protocols. Every MuseBox Server implementation has a common API based on ZeroMQ (aka ZMQ or \u00d8MQ or 0MQ), an embedded library for queue-based communication stack. The communication is based on the Publish - Subscribe classic pattern. The Senders of messages, called publishers (in our case, we can call it \"MuseBox Client\"), do not program the messages to be sent directly to specific receivers, called subscribers. Messages are published without the knowledge of what or if any subscriber of that knowledge exists. When MuseBox starts, exposes its communication socket as a Subscriber. When it receives a message from a MuseBox Client, it responds to the Publisher to the socket that the MuseBox Client exposes. In order to to that, the MuseBox Client send to the MuseBox Server message the address where it expects to receive the answer. The payload of the message is in JSON format. Let's see this example: A MuseBox Client send this message to the MuseBox Server: { \"topic\": \"FaceDetection\", \"image\": [...], \"imageHeight\": 250, \"imageWidth\": 250, \"publisherQueue\": \"tcp://*:5000\" } the MuseBox Server receives the message, and according to the field \"publisherQueue\", will open a socket to the address tcp://*:5000 and will send the response there. the MuseBox Server sends the reponse to the MuseBox Client: { \"data\": [ { \"face_BB\": { \"height\":62, \"width\":51, \"x\":262, \"y\":97 } } ], \"execution_time\":4.957312, \"publisherQueue\":\"tcp://*:5000\", \"status\":\"success\", \"topic\": \"FaceDetection\" } NB : in this documentation, the optional parameters will be written with this syntax: parameter?: <value>","title":"The Theory"},{"location":"api/general/#apis","text":"The MuseBox Server listens in localhost (127.0.0.1) at port 9696 in TCP protocol. According to that, in ZMQ socket binding, the Client will bind at this address for sending data to MuseBox Server: tcp://*:9696 All the APIs are in JSON format, and have the same structure. The structure is the following: { \"topic\": <string>, \"only_face\": <bool>, \"image\": <base64 image>, \"imageHeight\": <int>, \"imageWidth\": <int>, \"publisherQueue\": <string> } Where: \"topic\" is the name of the Machine Learning task \"image\" is the OpenCV image in base64 format (string encoded in UTF8) \"imageHeight\" is the height of the current image \"imageWidth\" is the width of the current image \"publisherQueue\" is the address where the Client expects to receive the response \"only_face\" (optional) if you want to execute only the selected task and not the dependent tasks. For example: if you want to execute the face recognition task, you need to execute before that the face detection for extracting the sub-image that contains the face of the person that you want to recognize; with only_face: true you are sure that the passed image contains only the cropped face, meanwhile without the field only_face , MuseBox Server executes face detection and face recognition. The MuseBox server response structure is the following: { \"data\": [ { \"<topic>_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], \"execution_time\": <double>, \"publisherQueue\": <string>, \"status\": <string>, \"topic\": <string> } Where: \"execution_time\" is the inference time on FPGA \"publisherQueue\" is the same string that the Client sent previously \"status\" is the status of the request (success / error) \"topic\" is the requested inference task \"data\" is the inference result in JSON Array, where: \"\\ _BB\" is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right) NB: if the selected task has dependent tasks (e.g. Face Recognition requires Face Detection) and you have not setted only_face: true , the field \"data\" contains all the dependent task executed. For example, Face Recognition without only_face: true : { \"data\": { \"FaceDetection\": { \"data\": [ { \"face_BB\": { \"height\": 71, \"width\": 65, \"x\": 292, \"y\": 56 } } ], \"status\": \"success\", \"topic\": \"FaceDetection\" }, \"FaceRecognition\": [ { \"clientId\": \"1\", \"personFound\": \"unknown\", \"status\": \"success\", \"topic\": \"FaceRecognition\" } ] } }","title":"APIs"},{"location":"api/general/#face-analysis","text":"","title":"Face Analysis"},{"location":"api/general/#face-detection","text":"It detects the faces in a scene up to 32\u00d732 pixel Request from Client: \"topic\": \"FaceDetection\" Response from Server: { \"data\": [ { \"face_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } face_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Face detection"},{"location":"api/general/#face-landmarking","text":"Given a face bounding box, it extracts the 98 relevant points of a face. Request from Client: \"topic\": \"FaceLandmark\" \"only_face\"?: true Response from Server: { \"data\": [ { \"landmarks\": [196 int] } ] } landmarks corresponds to (x,y) couples for face landmark points","title":"Face landmarking"},{"location":"api/general/#face-recognition","text":"Given a face bounding box and a database of faces, it recognizes a face in a scene You need to populate the directory /usr/local/bin/database/people with the photos of the desired people. Request from Client: \"topic\": \"FaceRecognition\" \"only_face\"?: true Response from Server: { \"data\": [ { \"personFound\": <string> } ] } personFound corresponds to the name of the person","title":"Face recognition"},{"location":"api/general/#glasses-detection","text":"Given a face bounding box, it detects whether a person wear glasses or not Request from Client: \"topic\": \"GlassesDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"glasses\": \"glasses\"/\"no glasses\" } ] } glasses is the result of inference (glasses / no glasses)","title":"Glasses detection"},{"location":"api/general/#eye-blink-detection","text":"Given a face bounding box, it detects where are the eyes and it determines if they are blinking or not Request from Client: \"topic\": \"EyeBlink\" \"only_face\"?: true Response from Server: { \"data\": [ { \"eyeState\": \"opened\"/\"closed\" } ] } eyeState is the result of inference (opened eyes / closed eyes)","title":"Eye Blink Detection"},{"location":"api/general/#age-detection","text":"Given a face bounding box, it determines the age of a person Request from Client: \"topic\": \"AgeDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"age\": <int> } ] } age corresponds to the age in an integer number between 0 - 100","title":"Age Detection"},{"location":"api/general/#gender-detection","text":"Given a face bounding box, it determines if the person is female or male Request from Client: \"topic\": \"GenderDetection\" \"only_face\"?: true Response from Server: { \"data\": [ { \"gender\": \"man\"/\"woman\" } ] } gender corresponds to the gender of the person (man or woman)","title":"Gender Detection"},{"location":"api/general/#people-analysis","text":"","title":"People Analysis"},{"location":"api/general/#people-detection","text":"It detects the people in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"PeopleDetection\" Response from Server: { \"data\": [ { \"person_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } people_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"People detection"},{"location":"api/general/#object-analysis","text":"","title":"Object Analysis"},{"location":"api/general/#object-detection","text":"It detects the objects in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"ObjectDetection\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Object detection"},{"location":"api/general/#logo-detection","text":"It detects the objects in a scene up to 64\u00d764 pixel Request from Client: \"topic\": \"LogoDetection\" Response from Server: { \"data\": [ { \"object_BB\": { \"height\": <int>, \"width\": <int>, \"x\": <int>, \"y\": <int> } } ], } object_BB is the bounding boxes of the task, according to OpenCV format (from top-left to bottom-right)","title":"Logo detection"},{"location":"api/general/#logo-recognition","text":"Given a logo bounding box, it determines the brand name You need to populate the directory /usr/local/bin/database/logo with the photos of the desired logos. Request from Client: \"topic\": \"LogoRecognition\" \"only_face\"?: true Response from Server: { \"data\": [ { \"logoFound\": <string> } ] } logoFound corresponds to the name of the logo","title":"Logo Recognition"},{"location":"api/nocode/","text":"NoCode Not implemented yet. Return in the next release :-)","title":"Generation from NoCode (experimental)"},{"location":"api/nocode/#nocode","text":"Not implemented yet. Return in the next release :-)","title":"NoCode"},{"location":"api/python/","text":"Python Example This example is available on the following link: https://s3.eu-central-1.wasabisys.com/musebox/musebox-client-examples-1.0.0.zip import cv2 import zmq import numpy as np import json import base64 def main(): # open camera context = zmq.Context() socket = context.socket(zmq.PUB) try: socket.bind(\"tcp://*:9696\") except zmq.error.ZMQError as e: print(e) socket2 = context.socket(zmq.SUB) socket2.connect(\"tcp://127.0.0.1:5556\") socket2.setsockopt_string(zmq.SUBSCRIBE, str('')) message = { \"topic\": \"FaceDetection\", \"clientId\": \"1\", \"publisherQueue\": \"tcp://*:5556\" } camera = cv2.VideoCapture(0) while True: # Capture the video frame ret, frame = camera.read() ret, image = cv2.imencode('.png', frame) encoded = base64.b64encode(image) message[\"image\"] = str(encoded, \"utf-8\") socket.send_string(json.dumps(message)) # waiting for response from server message = json.loads(socket2.recv()) print(message) # After the loop release the cap object camera.release() if __name__ == '__main__': main()","title":"Example Python"},{"location":"api/python/#python-example","text":"This example is available on the following link: https://s3.eu-central-1.wasabisys.com/musebox/musebox-client-examples-1.0.0.zip import cv2 import zmq import numpy as np import json import base64 def main(): # open camera context = zmq.Context() socket = context.socket(zmq.PUB) try: socket.bind(\"tcp://*:9696\") except zmq.error.ZMQError as e: print(e) socket2 = context.socket(zmq.SUB) socket2.connect(\"tcp://127.0.0.1:5556\") socket2.setsockopt_string(zmq.SUBSCRIBE, str('')) message = { \"topic\": \"FaceDetection\", \"clientId\": \"1\", \"publisherQueue\": \"tcp://*:5556\" } camera = cv2.VideoCapture(0) while True: # Capture the video frame ret, frame = camera.read() ret, image = cv2.imencode('.png', frame) encoded = base64.b64encode(image) message[\"image\"] = str(encoded, \"utf-8\") socket.send_string(json.dumps(message)) # waiting for response from server message = json.loads(socket2.recv()) print(message) # After the loop release the cap object camera.release() if __name__ == '__main__': main()","title":"Python Example"}]}